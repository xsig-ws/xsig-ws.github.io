<?xml version="1.0" encoding="euc-jp"?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="ja" lang="ja">

<head>
<link rel="STYLESHEET" href="misc/grichal.css" type="text/css" />
<meta http-equiv="Content-Type" content="text/html; charset=euc-jp" />
<title>Grid Challenge in SACSIS 2005</title>
</head>

<body>
<table class="main" >
<tr>
<td colspan="2" class="title">

<center>
<img src="misc/title.png" />
</center>
<h1>Grid Challenge in SACSIS 2005</h1>
</td>
</tr>

<tr>
<td class="main">

<h2 class="top">Grid Challenge : GXPのページ</h2>

<h3>はじめに</h3>

<div>
Grid Challengeで提供される環境(GCF)では、日常利用および並列プログラミ
ングのためのツールとしてGXPが提供、設定されます。その他のツールについ
ては、参加者用の<a href=info.html>トップページ</a>を御覧下さい。
</div>

<h3>GXPとはなんなんでしょうか?</h3>

<div>
<ul>

<li>Primaryには、多数のマシンをsshで同時にログインしてインタラクティブ
に使うためのツールです。コマンドをタイプすると、それが一斉に多数のノー
ドで起動する、というのが最もベーシックな機能です。

<li>分散環境における日常作業を効率化するのに使えるほか，単純なタスクであれば，
ほとんど並列プログラミングやネットワークプログラミングをすることなしに，
並列処理が簡単に行えます．

<li>デモ、マニュアル、などの情報は
<a href=http://www.logos.ic.i.u-tokyo.ac.jp/phoenix/gxp_quick_man_ja.shtml>
こちら</a>。

<li>GCF環境を使っているデモは<a href=misc/gxp-grch-demo.wmv>こちら</a>。

</ul>

</div>

<h3>今回の環境でどう使ったらいいのか手っ取り早く教えてくれませんか? </h3>

<div> GCFの各マシンにはすでにGXPが使えるようにインストールされています
ので，もっとも手っ取り早くは以下のようにします．

<ul>

<li>確認: 使い始める前に，参加者用の<a href=info.html>トップページ</a> 
をみて，入り口ホストにsshでログインできることを確認してください．また，
ssh-agentとssh-addを使って，入り口ホスト間でもログインができることを確
かめてください．

<li>どれか好きな入り口ホストを決めて，以下の手順でログインしてください．
以下ではistbs000.i.u-tokyo.ac.jpとします．

<pre>
your_host$ eval `ssh-agent`
your_host$ ssh-add
your_host$ ssh -A gr123@istbs000.i.u-tokyo.ac.jp
istbs000$ 
</pre>
gr123の部分はもちろん，委員会から割り当てられた名前を用いてください．
sshの-Aオプションはエージェント転送を有効にし、以降GXPホスト間を
渡り歩く時にパスフレーズを入れずに認証できるようにするために
必要です。

<li>
<p>無事ログインできたら，gxpコマンドを起動します．gxpは、全ノードにおいて、
~tau/local/Linux/bin/ にインストールされているので、起動したいホスト
で ~/.bashrc および ~/.bash_profile に以下を追加しておいて下さい。</p>
<pre>
export PATH=~tau/local/Linux/bin:$PATH
</pre>
その上でもう一度ログインし直すか、
<pre>
. ~/.bashrc
</pre>
として下さい。

<p>
注: このPATH追加はgxpを起動するホストでは必ず行なって下さい。
gxpはpythonで書かれており、一部のバージョンのpythonでは不具合
があることがわかっているため、正しいバージョンのpythonを使う意味からも
重要です。</p>
<pre>
istbs000$ which python
/home/tau/local/Linux/bin/python
</pre>
<p>などと表示されればOKです(/home/tauの部分はホストによって違うことがあります)。</p>

以上の準備のもと、gxpとタイプして下さい。
<pre>
istbs000$ gxp
GXP started on istbs000.i.u-tokyo.ac.jp
GXP[1/1/1] % 
</pre>
GCF環境用の標準設定ファイルが
<pre>
~tau/grch/conf.gxp
</pre>
です。
これを読み込むには、
GXPのrコマンドを用いて、
<pre>
GXP[1/1/1] % r ~tau/grch/conf.gxp
</pre>
とするか、gxpを立ち上げる際に引数に渡して、
<pre>
istbs000$ gxp ~tau/grch/conf.gxp
</pre>
としてください。または、同ファイルのコピーまたはシンボリックリンクを ~/.gxprc
という名前で保存すれば、起動時に自動的に読み込まれます。

この状態で、全ホストへログインできるように設定されています．
show planコマンドで確認して見てください．
<pre>
GXP[1/1/1] % show plan
...
 --> ...
 --> ...
 --> ...
</pre>
<p>ここでは，他の入り口ホストと、起動ホスト内のノード
がすべて列挙されていれば設定はうまくっています．
たとえば，istbs000にログインしているのであれば，他の入り口ホスト(nimbus, 
gk, poole, xcmp001, dennis-serv, bluecore, suidobashi00, shepherd)と，
istbs??? という名前の多数のノードが表示されればOKです．</p>

<p>exploreコマンドで，実際にそれらのホストにログインできます．</p>
<pre>
GXP[1/1/1] % explore
...
<色々なメッセージ>
...
GXP[115/115/115] % 
</pre>

<p>プロンプトには実際にログインできているホスト数の合計が表示されます
(3つの数字の詳細はマニュアルの2章を参照)。この場合、入り口ホスト数 + 
起動クラスタのノード数と同じ数が表示されれば成功です(極端に多くない限
り、あまり細かく数を気にする必要はないでしょう)．
</p>

<p>
GXPの基本コマンドはeコマンドで、これで
すべてのホストで同じコマンドを高速に起動することができます．
</p>

<pre>
GXP[115/115/115] % e hostname
... 
... <115のホスト名が表示される>
... 
</pre>

<p>
もう一度show planとすると，今度はGCF環境のすべてのノード(500ノード + 
入口ホスト + コンパイルetc.用ホスト)
が表示されるはずです．そしてexploreとすると，それらすべてにログインで
きます．
</p>

その他の設定ファイルのサンプルも用意しておきます。ディレクトリはすべて
~tau/grch です。
<ul>
<li>入口ノードだけにログインする設定ファイルはgw.gxpです。
<li>入口ノード + コンパイルホストにログインする設定ファイルは 
comp.gxpです。
<li>各クラスタから少数(10程度)台ずつ選ぶ設定ファイルは
conf_small.gxpです。
</ul>
<p>
実際にはすべて conf.gxp の一部を少し修正したものに過ぎません。
これらのサンプルファイルや
マニュアルなどを参照しながら自分で好きな設定を書けるようになるでしょう。</p>

<p>
その他の機能を箇条書きにしておきます．詳しくはホームページにあるマニュ
アルやデモビデオをご覧ください．</p>

<ul> 

<li>明らかに，eコマンドは，各クラスタごとに必要なコンパイルを一斉に行
うのに使えます．自分が作った並列プログラムを全ノードで一斉起動するのに
も使えます(以下，各コマンドについては<a
href=http://www.logos.ic.i.u-tokyo.ac.jp/phoenix/gxpmanj.html> マニュ
アル</a>3章「コマンドリファレンス」参照)．

<li>mwコマンドは，多数のプロセスの標準入出力を結合して起動します．これ
は簡単な並列タスクであれば，ほとんど並列プログラミングらしいことをせず
に実行する枠組みを提供します．それ以上に複雑なことをする場合でも，ネッ
トワークプログラムを最初に起動して，動的に割り当てられたポート番号など
を交換するのに使えます．

<li>exploreで多数のノードにログインした後、実際にコマンドを実行するノー
ドは、別の手段(smaskコマンド)で簡単に選択することができます。コンパイ
ルホストのみを選択する、入口ホストのみを選択する、一部のクラスタのみを
選択する、空いているノードを選択する、などの処理が対話的に、Unixコマン
ドを用いて可能です。本環境で必要になりそうな例として以下を ~tau/grch 下に
提供します。いずれも、多数のノードにログインした状態で、
<pre>
r ファイル名
</pre>
とすることで、一部のノードが以降の実行に選択されます。

<ul>
<li> sel_gw.gxp : 入口ホストを選択する。
<li> sel_compile.gxp : コンパイルホストを選択する。
<li> sel_nodes.gxp : ノードを選択する。
</ul>

<li>Ctrl-Cで現在実行中のプロセスをすべてのノードでkillできます．

<li>間違えて暴走プロセスなどをあちこちに残してしまった場合の最終兵器と
して，"皆殺しコマンド"bombがあります．クラスタを使う際の一種の精神安定
剤です．

<li>その他，cdや環境変数の設定などの通常のシェル的な作業ができます
(それも，多数ノードで一斉に)．

</ul>
</div>

<h3><font color=red>GXPで暴走/残骸プロセスを始末する方法</font></h3>
<div>
<p>GXPを用いて始末したいホストすべてにログインした上で、bombコマンドを発行して下さい。
すると、(起動ホストを除く)すべてのホストで、以下を満たすプロセスが始末されます。</p>
<ul>
<li>ユーザ名が自分である
<li>bombを発行したGXPプロセス(およびそれに必要なサブプロセス)ではない
</ul>
<p>
<font color=red>起動ホストでは何も殺されません。</font>
これは、無関係なエディタや関係のないターミナルなどを殺してしまうのを
防ぐためです。起動ホストでも構わず始末したい場合、rbombコマンドを使って下さい。
この場合もGXPの祖先となるプロセスは残されます。
</p>

<p>早い話が現在のGXPセッションを構成するプロセスを残して、残りすべてを削除します。
始末されたプロセスが表示されます。数秒して何も表示されずに帰ってくれば何も
残骸はなかったということになります。</p>

<p>これは、間違えてプロセスを暴走させてしまったり、デッドロックに陥っ
たプロセスを一気に処分する手軽な方法です。一般論として、プロセスを何ら
かの理由で一部強制的に終了などした場合、他のホストでは走り続けたり、デッ
ドロックしたまま残っている可能性が高いですから、ときどき、習慣として行
なうことを進めます(GXPを使っていて途中でフリーズしてしまった場合も、残
骸プロセスが残っている場合があります)。</p>

<p>原理はpsコマンドを用いて関係ないプロセスを見つけ、killコマンドでkillして
いるだけですので、そのプロセスがGXPを用いて作られた物であるかどうかは関係なく、
始末の対象となります。</p>

<p>グリチャレ特有の事情として、同じログイン名で複数の人が同時に作業を
している時は、他人のプロセスを殺してしまうかも知れないので気をつけて下
さい。</p>

<p>例えば、GCF環境すべてのノードでプロセスを始末する場合、手順は以下の通
りです。入口としてistbs000を利用するとします。<a href=misc/bomb.wmv>ビ
デオによるデモ</a>もあります。</p>

<pre>
yourhost$ eval `ssh-agent`
yourhost$ ssh-add
...
yourhost$ ssh -A grchXXX@istbs000.i.u-tokyo.ac.jp
istbs000% gxp   (注: ~tau/local/Linux/bin/gxp)
GXP[1/1/1]% r ~tau/grch/conf.gxp
GXP[1/1/1]% explore
	...
	...
GXP[108/108/108]% bomb
	...
	...
GXP[108/108/108]% explore
	...
	...
GXP[511/511/511]% bomb
	...
	...
</pre>

<p>どのクラスタに不始末プロセスがいるかわかっている場合などに、全部に
いちいちログインするのは時間がかかって面倒という場合があるかも知れません。
GXPを使いこなせば自分でログインするホストを自由に選べますが、
~tau/grch/以下に、各クラスタにログインするconfig fileを用意しておきました。
例えば、
</p>
<pre>
GXP[1/1/1]% r ~tau/grch/conf.gxp
</pre>
の変わりに、
<pre>
GXP[1/1/1]% r ~tau/grch/istbs.gxp
</pre>
とすれば、istbsにのみログインする設定が読み込まれます。以下の設定を用意しておきました。
<ul>
<li>alab.gxp 
<li>hirakken.gxp
<li>hpcs.gxp
<li>istbs.gxp
<li>matsuken.gxp
<li>sheep.gxp
<li>tok.gxp
<li>xcluster.gxp
<li>yubahon.gxp 
</ul>

</div>


<h3>いくつかの必要な知恵</h3>
<div>
<ul>

<li>不幸にして，時々GXPが固まることがあります．そうなってしまったら，
もう一度別の端末から同じホスト群にログインし，bombコマンド使って残骸プ
ロセスをクリーンアップしましょう。基本的には現在のGXPセッション以外の
すべてのプロセス(もちろん自分のユーザIDのもののみ)をkill -9します(詳細
はマニュアル参照)。グリチャレ環境では同じアカウントを複数の人が共有し
ているので、同時に使っている場合は気をつけて下さい。

<li>exploreで，あるノードへのログインが失敗した場合，それは故障による
ものの場合もあれば，設定ミスの場合もあります．explore中のメッセージを
読むことである程度その原因を探ることができます．

設定ミスであれば、直してもう一度exploreします。

本当の故障であればそれらへのログインを試みないようにします。
<pre>
GXP[..] % des <故障ホスト名>
</pre>
とするとそれらのホストへはもうログインを試みません。失敗したノード
すべてをdesしたい場合、
<pre>
GXP[..] % desf
</pre>
としてください。

</ul>

</div>


</td>


<td class="menu" valign="top">

<h3 class="menu">menu</h3>
<div class="menu">
<ul class="menu">
<li><a href="index.html" class="menu_d">Top</a></li>
<li><a href="regulation.html" class="menu_d">規定課題</a></li>
<li><a href="reg_detail.html" class="menu_d_sub">規定課題詳細</a></li>
<li><a href="reg_api_c.html" class="menu_d_sub">出題API(C言語用)</a></li>
<li><a href="reg_api_java.html" class="menu_d_sub">出題API(Java用)</a></li>
<li><a href="reg_trial.html" class="menu_d_sub">お試しパック</a></li>
<li><a href="reg_elimination.html" class="menu_d_sub">予選詳細</a></li>
<li><a href="reg_el_result.html" class="menu_d_sub">予選結果</a></li>
<li><a href="reg_final.html" class="menu_d_sub">本選詳細</a></li>
<li><a href="reg_fin_result.html" class="menu_d_sub">本選結果</a></li>
<li><a href="reg_fin_data.html" class="menu_d_sub">本選出題内容</a></li>
<li><a href="free_regulation.html" class="menu_d">自由課題</a></li>
<hr />
<li><a href="http://www.hpcc.jp/sacsis/2005/" class="menu_d">SACSIS 2005</a></li>
</ul>
</div>
<h3 class="menu">what's new</h3>
<div class="menu">

<ul class="small">
<li>2005/3/22 決勝終了。集計結果は<a href=reg_fin_result.html>こちら</a>。出題データに関する資料は<a href=reg_fin_data.html>こちら。</a>
<li>2005/3/11 決勝で用いたプログラムソースの提出方法について<a href=reg_final.html>追加</a>しました。
<li>2005/3/10 決勝の実行を収めた動画キャプチャの提出方法について<a href=reg_final.html>更新</a>しました。
<li>2005/3/02 予選中間結果を<a href=reg_el_result.html>更新</a>しました。
<li>2005/2/09 予選中間結果を<a href=reg_el_result.html>公開</a>しました。
<li>2005/2/02 予選問題に関する詳細情報を<a href=reg_elimination.html>公開</a>しました。
<li>2005/1/30 ホスト情報の最終版を参加者へ通知しました。
<li>2005/1/30 参加チーム情報を<a href=grch_meibo.html>公開</a>しました。

<li>
<font color=red>2005/1/27 
予選スケジュールを延期しました。詳しくは<a href=regulation.html#schedule>こちら</a>
</font>
<li>2005/1/27 
<a href=awards.html>賞に関する情報</a>を掲載するページを作成しました。
<li>
2005/1/19 参加チームにホスト情報の提供を開始しました
<li><a href=http://www.atmarkit.co.jp/index.html>
2005/1/7 @IT</a>に掲載されました
(<a href=http://www.atmarkit.co.jp/news/200501/08/grid.html>
1000CPUでグリッドを体感、「Grid Challenge」開催へ</a>)</li>
<li> 2005/1/7 Grid協議会「グリッドショーケース」にて講演を行ないました
<li> 2005/1/1 規定問題詳細とお試しパック公開!</li>
<li> 2004/12/15 <a href=http://nikkeibp.jp/wcs/leaf/CID/onair/jp/sangaku/349589>nikkeibp.jp</a>に掲載されました</li>
<li> 2004/12/7 サイト開設</li>
<li> 2004/12/7 参加者募集開始! </li>
</ul>
</div>
</td>

</tr>



<tr>
<td colspan="2" class="footer">
<hr />
Grid Challenge in SACSIS 2005
</td>
</table>

</body>
</html>
